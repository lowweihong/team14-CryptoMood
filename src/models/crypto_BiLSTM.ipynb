{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5kxNHQS4twH",
        "outputId": "ba75f9ef-5af5-42ae-e9c5-d58ce6ae8ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import os\n",
        "import re\n",
        "from transformers import TrainerCallback\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "torch.manual_seed(24)\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "#!pip install datasets\n",
        "\n",
        "def remove_wallets(text):\n",
        "    # This is a basic implementation - modify if your original remove_wallets was different\n",
        "    # Common crypto wallet patterns (like Bitcoin/Ethereum addresses)\n",
        "    wallet_pattern = r'0x[a-fA-F0-9]{40}|[13][a-km-zA-HJ-NP-Z1-9]{25,34}'\n",
        "    return re.sub(wallet_pattern, '', text)\n",
        "\n",
        "# Apply cleaning operations specified to the paper, https://github.com/mikik1234/CryptoBERT-LUKE/blob/main/CODE_Data_Collection.ipynb\n",
        "def clean_text(text):\n",
        "    # Remove Asian characters\n",
        "    text = re.sub(r'[\\u4e00-\\u9fff]+', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    # Remove mentions, hashtags, stock symbols, and forward slashes with content\n",
        "    text = re.sub(r'[@][A-Za-z0-9_]+|#[A-Za-z0-9_]+|$[A-Za-z0-9_ ]+|/[A-Za-z0-9_ ]+', '', text)\n",
        "    # Remove RT prefix\n",
        "    text = re.sub(r'RT : ', '', text)\n",
        "    # Replace & with 'and'\n",
        "    text = re.sub(r'&', 'and', text)\n",
        "    # Handle special characters and quotes\n",
        "    text = re.sub(r'â€™', '\\'', text)\n",
        "    text = re.sub(r'[\"&;]', '', text)\n",
        "    text = re.sub(r'', '', text)  # Zero-width space\n",
        "    # Remove .X or .x\n",
        "    text = re.sub(r'\\.[Xx]', '', text)\n",
        "    # Normalize multiple dots to ellipsis\n",
        "    text = re.sub(r'\\.\\.+', '...', text)\n",
        "    # Remove standalone @ and pipe symbols\n",
        "    text = re.sub(r'@|\\|', '', text)\n",
        "    # Normalize spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove wallet addresses\n",
        "    text = remove_wallets(text)\n",
        "    text = re.sub(r'^\\s*\\S+(?:\\s+\\S+){0,2}\\s*$', '', text) # Remove short texts (fewer than 4 words)\n",
        "    return text\n",
        "\n",
        "def sentiment_map(text):\n",
        "  if 'Bullish' in text:\n",
        "    return 0\n",
        "  elif 'Neutral' in text:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "LWb7-8sD1lc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Create a generator for the split\n",
        "generator = torch.Generator().manual_seed(seed)"
      ],
      "metadata": {
        "id": "dmHkB6M51m3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"StephanAkkerman/financial-tweets-crypto\")\n",
        "train_dataset_ori = data['train']"
      ],
      "metadata": {
        "id": "uvcpzpc31p-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'No. of data: {len(train_dataset_ori)}')\n",
        "train_dataset_ori = train_dataset_ori.filter(lambda data: data['sentiment'] is not None)\n",
        "print(f'No. of data after remove sentiment equals to none: {len(train_dataset_ori)}')\n",
        "train_dataset_ori = train_dataset_ori.filter(lambda data: data['tweet_type']!='quote tweet')\n",
        "print(f'No. of data after remove quote tweet: {len(train_dataset_ori)}')\n",
        "train_dataset_ori = train_dataset_ori.filter(lambda data: len(data['description'].split(' '))>1)\n",
        "print(f'No. of data after remove short text: {len(train_dataset_ori)}')\n",
        "train_dataset_ori = train_dataset_ori.to_pandas()\n",
        "train_dataset_ori['description'] = train_dataset_ori['description'].apply(clean_text)\n",
        "train_dataset_ori.drop_duplicates(inplace=True, ignore_index=True)\n",
        "print(f'No. of data after remove duplicates: {len(train_dataset_ori)}') # Make sure the records here remains the same after remove duplicates, else the following train test split might be different\n",
        "train_dataset_ori['sentiment_label'] = train_dataset_ori['sentiment'].apply(sentiment_map)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukIU5mhF1sCD",
        "outputId": "bbbabcdd-cabb-4fd3-e035-629bc04a0354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of data: 57935\n",
            "No. of data after remove sentiment equals to none: 48692\n",
            "No. of data after remove quote tweet: 46866\n",
            "No. of data after remove short text: 45567\n",
            "No. of data after remove duplicates: 45567\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = len(train_dataset_ori)\n",
        "# Create an array of indices\n",
        "indices = np.arange(num_samples)\n",
        "\n",
        "# Shuffle the indices randomly\n",
        "np.random.seed(42)  # Set a seed for reproducibility\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Split the indices into train, validation, and test sets\n",
        "train_size = int(num_samples * 0.8)  # 80% for training\n",
        "val_size = int(num_samples * 0.1)  # 10% for validation\n",
        "test_size = num_samples - train_size - val_size  # 10% for testing\n",
        "\n",
        "# Split the shuffled indices\n",
        "train_idx = indices[:train_size]\n",
        "val_idx = indices[train_size:train_size + val_size]\n",
        "test_idx = indices[train_size + val_size:]\n",
        "\n",
        "# Print the sizes of each split\n",
        "print(f\"Train size: {len(train_idx)}\")\n",
        "print(f\"Validation size: {len(val_idx)}\")\n",
        "print(f\"Test size: {len(test_idx)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-gKoA2Y1uFG",
        "outputId": "8022e67b-84a9-465b-8a87-65b9d933c547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 36453\n",
            "Validation size: 4556\n",
            "Test size: 4558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset_ori.loc[train_idx]\n",
        "valid_dataset = train_dataset_ori.loc[val_idx]\n",
        "test_dataset = train_dataset_ori.loc[test_idx]"
      ],
      "metadata": {
        "id": "VycyTVde11fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare the data\n",
        "#sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}  # Adjust based on your actual sentiment values\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.texts = texts\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        # print(self.texts[idx])\n",
        "        item['text'] = self.texts[idx]\n",
        "        # item['text'] = torch.tensor(self.texts[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "model_name = \"ElKulako/cryptobert\"\n",
        "\n",
        "# 3. Initialize tokenizer\n",
        "#tokenizer = BertTokenizer.from_pretrained('ElKulako/cryptobert')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# 6. Create dataset\n",
        "encodings = tokenizer(train_dataset['description'].to_list(),\n",
        "                      truncation=True, padding=\"max_length\", max_length=128)\n",
        "train_dataset = TweetDataset(train_dataset['description'].to_list(), encodings, train_dataset['sentiment_label'].to_list())\n",
        "encodings = tokenizer(valid_dataset['description'].to_list(),\n",
        "                      truncation=True, padding=\"max_length\", max_length=128)\n",
        "val_dataset = TweetDataset(valid_dataset['description'].to_list(), encodings, valid_dataset['sentiment_label'].to_list())\n",
        "encodings = tokenizer(test_dataset['description'].to_list(),\n",
        "                      truncation=True, padding=\"max_length\", max_length=128)\n",
        "test_dataset = TweetDataset(test_dataset['description'].to_list(), encodings, test_dataset['sentiment_label'].to_list())\n"
      ],
      "metadata": {
        "id": "MglIVkuu13Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set your desired batch size\n",
        "batch_size = 128\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "x2HiQgOf18NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes, num_layers=1, dropout=0.5, bidirectional=True):\n",
        "        super(BiLSTMClassifier, self).__init__()\n",
        "        # Set padding_idx to ensure the embedding for padding tokens remains constant (if your tokenizer defines one)\n",
        "        padding_idx = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=padding_idx)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        # If using bidirectional LSTM, the final hidden size will be doubled.\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(lstm_output_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids shape: (batch_size, seq_length)\n",
        "        embedded = self.embedding(input_ids)  # (batch_size, seq_length, embed_dim)\n",
        "\n",
        "        # Pass through LSTM\n",
        "        # You could also use pack_padded_sequence here if you wish to handle variable lengths.\n",
        "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
        "\n",
        "        # Concatenate the final forward and backward hidden states\n",
        "        if self.lstm.bidirectional:\n",
        "            h_n_cat = torch.cat((h_n[-2], h_n[-1]), dim=1)  # shape: (batch_size, hidden_dim*2)\n",
        "        else:\n",
        "            h_n_cat = h_n[-1]  # shape: (batch_size, hidden_dim)\n",
        "\n",
        "        dropped = self.dropout(h_n_cat)\n",
        "        logits = self.fc(dropped)  # shape: (batch_size, num_classes)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "4QBB-TSH3Ec7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "embed_dim = 128     # Dimension for word embeddings; experiment as necessary\n",
        "hidden_dim = 256    # Size of LSTM hidden states\n",
        "num_classes = 3     # According to your sentiment_map (0: Bullish, 1: Neutral, 2: Bearish)\n",
        "num_layers = 2      # You may experiment with deeper networks\n",
        "dropout = 0.3\n",
        "bidirectional = True\n",
        "\n",
        "model = BiLSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, num_layers, dropout, bidirectional)\n"
      ],
      "metadata": {
        "id": "yVd4ySk93FpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n"
      ],
      "metadata": {
        "id": "0c72n91o3Jbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7yXxEZ73LdA",
        "outputId": "fea991de-939d-4c3a-99c3-2ca301079ff1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTMClassifier(\n",
              "  (embedding): Embedding(50265, 128, padding_idx=1)\n",
              "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Assuming your model is defined as `model`\n",
        "print(f\"Number of trainable parameters: {count_trainable_params(model)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPPkTXw630gN",
        "outputId": "539f38bb-0084-400f-98fe-8fc11bf0bca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 8802947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            # Move inputs and labels to the device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "    return avg_loss, accuracy, f1, precision\n",
        "\n",
        "num_epochs = 5  # Adjust as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    val_loss, val_acc, val_f1, val_precision = evaluate(model, val_dataloader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Accuracy: {val_acc:.4f} | F1: {val_f1:.4f} | Precision: {val_precision:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4thSBggm3Mha",
        "outputId": "9cbb4567-e4e8-47cc-8068-e8704b7b4e24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5:\n",
            "  Train Loss: 0.8337\n",
            "  Val Loss:   0.7894 | Accuracy: 0.6466 | F1: 0.5843 | Precision: 0.6261\n",
            "Epoch 2/5:\n",
            "  Train Loss: 0.7261\n",
            "  Val Loss:   0.7148 | Accuracy: 0.6811 | F1: 0.6544 | Precision: 0.6658\n",
            "Epoch 3/5:\n",
            "  Train Loss: 0.6343\n",
            "  Val Loss:   0.6952 | Accuracy: 0.6978 | F1: 0.6647 | Precision: 0.6909\n",
            "Epoch 4/5:\n",
            "  Train Loss: 0.5552\n",
            "  Val Loss:   0.7214 | Accuracy: 0.7098 | F1: 0.6923 | Precision: 0.7030\n",
            "Epoch 5/5:\n",
            "  Train Loss: 0.4728\n",
            "  Val Loss:   0.7430 | Accuracy: 0.7133 | F1: 0.7041 | Precision: 0.7037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_f1, test_precision = evaluate(model, test_dataloader, criterion)\n",
        "print(\"Test Metrics:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  F1 Score: {test_f1:.4f}\")\n",
        "print(f\"  Precision: {test_precision:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MireLOBv3Zb1",
        "outputId": "c03992db-2c15-484e-9e0d-1b28b9219212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "  Loss: 0.7347\n",
            "  Accuracy: 0.7095\n",
            "  F1 Score: 0.7002\n",
            "  Precision: 0.6999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Disable gradient calculation for efficiency\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move inputs and labels to the selected device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(input_ids)\n",
        "            # Get the predicted class (index) for each batch sample\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            # Collect results for the entire test set\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate the overall accuracy\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    # Calculate the precision for each class (order: 0: Bullish, 1: Neutral, 2: Bearish)\n",
        "    precisions = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
        "    # Calculate the weighted F1-score\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "\n",
        "    print(\"Evaluation Metrics:\")\n",
        "    print(f\"Precision (Bullish): {precisions[0]:.4f}\")\n",
        "    print(f\"Precision (Neutral): {precisions[1]:.4f}\")\n",
        "    print(f\"Precision (Bearish): {precisions[2]:.4f}\")\n",
        "    print(f\"Accuracy:          {acc:.4f}\")\n",
        "    print(f\"F1-Score:          {f1:.4f}\")\n",
        "\n",
        "# Assuming `model` is your trained BiLSTM model, `test_dataloader` is defined, and `device` is set:\n",
        "evaluate_model(model, test_dataloader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aPCT8hA8NEx",
        "outputId": "7410ce47-f517-4f69-f3f3-fe69349b0cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Metrics:\n",
            "Precision (Bullish): 0.7656\n",
            "Precision (Neutral): 0.5848\n",
            "Precision (Bearish): 0.6007\n",
            "Accuracy:          0.7095\n",
            "F1-Score:          0.7002\n"
          ]
        }
      ]
    }
  ]
}
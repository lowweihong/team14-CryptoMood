{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSw50GT5hH9F",
        "outputId": "71d1d7fe-ae29-4957-fbc2-c2b0326417f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import os\n",
        "import re\n",
        "from transformers import TrainerCallback\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "#!pip install datasets\n",
        "\n",
        "def clean_text_list(text_list):\n",
        "    # Handle None values and convert all to strings\n",
        "    cleaned_list = [str(text) if text is not None else \"\" for text in text_list]\n",
        "\n",
        "    # Helper function to remove wallet addresses (assuming this is what remove_wallets does)\n",
        "    def remove_wallets(text):\n",
        "        # This is a basic implementation - modify if your original remove_wallets was different\n",
        "        # Common crypto wallet patterns (like Bitcoin/Ethereum addresses)\n",
        "        wallet_pattern = r'0x[a-fA-F0-9]{40}|[13][a-km-zA-HJ-NP-Z1-9]{25,34}'\n",
        "        return re.sub(wallet_pattern, '', text)\n",
        "\n",
        "    # Apply cleaning operations\n",
        "    def clean_text(text):\n",
        "        # Remove Asian characters\n",
        "        text = re.sub(r'[\\u4e00-\\u9fff]+', '', text)\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "        # Remove mentions, hashtags, stock symbols, and forward slashes with content\n",
        "        #text = re.sub(r'[@][A-Za-z0-9_]+|#[A-Za-z0-9_]+|\\$[A-Za-z0-9_ ]+|/[A-Za-z0-9_ ]+', '', text)\n",
        "        # Remove RT prefix\n",
        "        text = re.sub(r'RT : ', '', text)\n",
        "        # Replace & with 'and'\n",
        "        text = re.sub(r'&', 'and', text)\n",
        "        # Handle special characters and quotes\n",
        "        text = re.sub(r'â€™', '\\'', text)\n",
        "        text = re.sub(r'[\"&;]', '', text)\n",
        "        text = re.sub(r'', '', text)  # Zero-width space\n",
        "        # Remove .X or .x\n",
        "        text = re.sub(r'\\.[Xx]', '', text)\n",
        "        # Normalize multiple dots to ellipsis\n",
        "        text = re.sub(r'\\.\\.+', '...', text)\n",
        "        # Remove standalone @ and pipe symbols\n",
        "        text = re.sub(r'@|\\|', '', text)\n",
        "        # Normalize spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove wallet addresses\n",
        "        text = remove_wallets(text)\n",
        "        return text\n",
        "\n",
        "    # Apply cleaning to all texts\n",
        "    cleaned_list = [clean_text(text) for text in cleaned_list]\n",
        "\n",
        "    # # Remove duplicates and filter by minimum word count (4 words)\n",
        "    # seen = set()\n",
        "    # result = []\n",
        "    # for text in cleaned_list:\n",
        "    #     if text and text not in seen and len(text.split()) >= 4:\n",
        "    #         seen.add(text)\n",
        "    #         result.append(text)\n",
        "\n",
        "    return cleaned_list\n",
        "\n",
        "def sentiment_map(text):\n",
        "  if 'Bullish' in text:\n",
        "    return 0\n",
        "  elif 'Neutral' in text:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "gFcoEpRwhTUm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"StephanAkkerman/financial-tweets-crypto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPKvTPWwhYXH",
        "outputId": "d34edfa3-9c2e-4fd2-ef1f-33ac64e95bfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_ori = data['train']\n",
        "train_dataset_ori = train_dataset_ori.filter(lambda data: data['sentiment'] is not None)\n"
      ],
      "metadata": {
        "id": "fiTAJJOOhabW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare the data\n",
        "#sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}  # Adjust based on your actual sentiment values\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "model_name = \"kk08/CryptoBERT\"\n",
        "\n",
        "# 3. Initialize tokenizer\n",
        "#tokenizer = BertTokenizer.from_pretrained('ElKulako/cryptobert')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# 4. Prepare texts and labels with cleaning\n",
        "texts = train_dataset_ori['description']#clean_text_list(train_dataset_ori['description'])#[str(t) if t is not None else \"\" for t in train_dataset['description']]  # Convert None to empty string and ensure all are strings\n",
        "labels = [sentiment_map(sent_label) for sent_label in train_dataset_ori['sentiment']]  # Default to neutral if unknown\n",
        "\n",
        "# 5. Tokenize the texts\n",
        "# Filter out empty strings and keep track of valid indices\n",
        "# valid_texts = [t for t in texts if t.strip()]\n",
        "# valid_labels = [labels[i] for i, t in enumerate(texts) if t.strip()]\n",
        "encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "# 6. Create dataset\n",
        "tweet_dataset = TweetDataset(encodings, labels)\n",
        "\n",
        "# 7. Split into train and validation (80-20 split)\n",
        "train_size = int(0.8 * len(tweet_dataset))\n",
        "val_size = len(tweet_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(tweet_dataset, [train_size, val_size])\n",
        "\n",
        "# 8. Initialize model\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,  # Set to 3 for Negative, Neutral, Positive\n",
        "    ignore_mismatched_sizes=True  # This allows us to override the 2-label head\n",
        ")\n",
        "\n",
        "\n",
        "# # 12. Train the model\n",
        "# trainer.train()\n",
        "\n",
        "# # 13. Evaluate on test set\n",
        "# test_results = trainer.evaluate(val_dataset)\n",
        "# print(\"\\nTest set evaluation results:\")\n",
        "# print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "# print(f\"Test loss: {test_results['eval_loss']:.4f}\")\n",
        "\n",
        "\n",
        "# # # 13. Save the model\n",
        "# model.save_pretrained(\"./trained_bert_model\")\n",
        "# tokenizer.save_pretrained(\"./trained_bert_model\")\n",
        "\n",
        "# print(\"Training completed! Model saved to './trained_bert_model'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D1vI-CUhb7w",
        "outputId": "043456d9-04d1-442a-e1e8-029b6251e5fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kk08/CryptoBERT and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the output size (number of classes)\n",
        "print(\"Number of output classes:\", model.classifier.in_features)\n",
        "\n",
        "print(\"Number of output classes:\", model.classifier.out_features)\n",
        "\n",
        "print(model.config.hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8oi385vZUm",
        "outputId": "d791f182-7147-484c-d80a-99f706c56854"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of output classes: 768\n",
            "Number of output classes: 3\n",
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"no\",\n",
        "    # load_best_model_at_end=True,\n",
        "    # eval_accumulation_steps=1,\n",
        "    report_to=None,          # Disable external logging (e.g., WANDB),\n",
        "    logging_steps=0.3,\n",
        "    log_level='info',\n",
        "    # prediction_loss_only=True\n",
        ")\n",
        "\n",
        "# 10. Define compute metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    accuracy = (preds == labels).mean()\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "# 11. Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 12. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "test_results = trainer.evaluate(val_dataset)\n",
        "print(\"\\nTest set evaluation results:\")\n",
        "print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Test loss: {test_results['eval_loss']:.4f}\")\n",
        "\n",
        "\n",
        "# # 13. Save the model\n",
        "model.save_pretrained(\"./trained_bert_model\")\n",
        "tokenizer.save_pretrained(\"./trained_bert_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 977
        },
        "id": "aa8d9a7qvvw8",
        "outputId": "a4c36aaa-6293-4648-ac77-ac0703a5e372"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "***** Running training *****\n",
            "  Num examples = 38,953\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 915\n",
            "  Number of trainable parameters = 109,484,547\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [915/915 44:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.678100</td>\n",
              "      <td>0.602969</td>\n",
              "      <td>0.730979</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.508700</td>\n",
              "      <td>0.599465</td>\n",
              "      <td>0.745046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.399000</td>\n",
              "      <td>0.630659</td>\n",
              "      <td>0.749666</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [77/77 01:14]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./trained_bert_model/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set evaluation results:\n",
            "Test accuracy: 0.7526\n",
            "Test loss: 0.6249\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in ./trained_bert_model/model.safetensors\n",
            "tokenizer config file saved in ./trained_bert_model/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_bert_model/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./trained_bert_model/tokenizer_config.json',\n",
              " './trained_bert_model/special_tokens_map.json',\n",
              " './trained_bert_model/vocab.txt',\n",
              " './trained_bert_model/added_tokens.json',\n",
              " './trained_bert_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvMth0xMf1Az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
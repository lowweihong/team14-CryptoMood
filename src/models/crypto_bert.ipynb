{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "igL9m_hJpNoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import os\n",
        "import re\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "#!pip install datasets\n",
        "\n",
        "def clean_text_list(text_list):\n",
        "    # Handle None values and convert all to strings\n",
        "    cleaned_list = [str(text) if text is not None else \"\" for text in text_list]\n",
        "\n",
        "    # Helper function to remove wallet addresses (assuming this is what remove_wallets does)\n",
        "    def remove_wallets(text):\n",
        "        # This is a basic implementation - modify if your original remove_wallets was different\n",
        "        # Common crypto wallet patterns (like Bitcoin/Ethereum addresses)\n",
        "        wallet_pattern = r'0x[a-fA-F0-9]{40}|[13][a-km-zA-HJ-NP-Z1-9]{25,34}'\n",
        "        return re.sub(wallet_pattern, '', text)\n",
        "\n",
        "    # Apply cleaning operations\n",
        "    def clean_text(text):\n",
        "        # Remove Asian characters\n",
        "        text = re.sub(r'[\\u4e00-\\u9fff]+', '', text)\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "        # Remove mentions, hashtags, stock symbols, and forward slashes with content\n",
        "        #text = re.sub(r'[@][A-Za-z0-9_]+|#[A-Za-z0-9_]+|\\$[A-Za-z0-9_ ]+|/[A-Za-z0-9_ ]+', '', text)\n",
        "        # Remove RT prefix\n",
        "        text = re.sub(r'RT : ', '', text)\n",
        "        # Replace & with 'and'\n",
        "        text = re.sub(r'&', 'and', text)\n",
        "        # Handle special characters and quotes\n",
        "        text = re.sub(r'â€™', '\\'', text)\n",
        "        text = re.sub(r'[\"&;]', '', text)\n",
        "        text = re.sub(r'', '', text)  # Zero-width space\n",
        "        # Remove .X or .x\n",
        "        text = re.sub(r'\\.[Xx]', '', text)\n",
        "        # Normalize multiple dots to ellipsis\n",
        "        text = re.sub(r'\\.\\.+', '...', text)\n",
        "        # Remove standalone @ and pipe symbols\n",
        "        text = re.sub(r'@|\\|', '', text)\n",
        "        # Normalize spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Remove wallet addresses\n",
        "        text = remove_wallets(text)\n",
        "        return text\n",
        "\n",
        "    # Apply cleaning to all texts\n",
        "    cleaned_list = [clean_text(text) for text in cleaned_list]\n",
        "\n",
        "    # # Remove duplicates and filter by minimum word count (4 words)\n",
        "    # seen = set()\n",
        "    # result = []\n",
        "    # for text in cleaned_list:\n",
        "    #     if text and text not in seen and len(text.split()) >= 4:\n",
        "    #         seen.add(text)\n",
        "    #         result.append(text)\n",
        "\n",
        "    return cleaned_list\n",
        "\n",
        "def sentiment_map(text):\n",
        "  if 'Bullish' in text:\n",
        "    return 0\n",
        "  elif 'Neutral' in text:\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"StephanAkkerman/financial-tweets-crypto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0q93SL0qDV7",
        "outputId": "6f831310-04e1-4999-f5ba-24a1fa616359"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_ori = data['train']\n",
        "train_dataset_ori = train_dataset_ori.filter(lambda data: data['sentiment'] is not None)\n"
      ],
      "metadata": {
        "id": "epRPzBKhqH9B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Prepare the data\n",
        "#sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}  # Adjust based on your actual sentiment values\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "model_name = \"ElKulako/cryptobert\"\n",
        "\n",
        "# 3. Initialize tokenizer\n",
        "#tokenizer = BertTokenizer.from_pretrained('ElKulako/cryptobert')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# 4. Prepare texts and labels with cleaning\n",
        "texts = clean_text_list(train_dataset_ori['description'])#[str(t) if t is not None else \"\" for t in train_dataset['description']]  # Convert None to empty string and ensure all are strings\n",
        "labels = [sentiment_map(sent_label) for sent_label in train_dataset_ori['sentiment']]  # Default to neutral if unknown\n",
        "\n",
        "# 5. Tokenize the texts\n",
        "# Filter out empty strings and keep track of valid indices\n",
        "# valid_texts = [t for t in texts if t.strip()]\n",
        "# valid_labels = [labels[i] for i, t in enumerate(texts) if t.strip()]\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# 6. Create dataset\n",
        "tweet_dataset = TweetDataset(encodings, labels)\n",
        "\n",
        "# 7. Split into train and validation (80-20 split)\n",
        "train_size = int(0.8 * len(tweet_dataset))\n",
        "val_size = len(tweet_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(tweet_dataset, [train_size, val_size])\n",
        "\n",
        "# 8. Initialize model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
        "\n",
        "# 9. Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=128,\n",
        "    per_device_eval_batch_size=128,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"no\",\n",
        "    # load_best_model_at_end=True,\n",
        "    # eval_accumulation_steps=1,\n",
        "    report_to=None,          # Disable external logging (e.g., WANDB),\n",
        "    logging_steps=0.3,\n",
        "    log_level='info',\n",
        "    # prediction_loss_only=True\n",
        ")\n",
        "\n",
        "# 10. Define compute metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    accuracy = (preds == labels).mean()\n",
        "    return {'accuracy': accuracy}\n",
        "\n",
        "# 11. Create Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 12. Train the model\n",
        "trainer.train()\n",
        "\n",
        "# 13. Evaluate on test set\n",
        "test_results = trainer.evaluate(val_dataset)\n",
        "print(\"\\nTest set evaluation results:\")\n",
        "print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")\n",
        "print(f\"Test loss: {test_results['eval_loss']:.4f}\")\n",
        "\n",
        "\n",
        "# # 13. Save the model\n",
        "model.save_pretrained(\"./trained_bert_model\")\n",
        "tokenizer.save_pretrained(\"./trained_bert_model\")\n",
        "\n",
        "# print(\"Training completed! Model saved to './trained_bert_model'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h0_yUfF0qgVH",
        "outputId": "c9928eda-15e4-416c-f56e-5f018945c6cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Safetensors PR exists\n",
            "***** Running training *****\n",
            "  Num examples = 38,953\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 915\n",
            "  Number of trainable parameters = 124,647,939\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='915' max='915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [915/915 37:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.716100</td>\n",
              "      <td>0.592122</td>\n",
              "      <td>0.743608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.532000</td>\n",
              "      <td>0.586420</td>\n",
              "      <td>0.752747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>0.415500</td>\n",
              "      <td>0.624481</td>\n",
              "      <td>0.759626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 9739\n",
            "  Batch size = 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='77' max='77' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [77/77 00:56]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Configuration saved in ./trained_bert_model/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set evaluation results:\n",
            "Test accuracy: 0.7594\n",
            "Test loss: 0.6205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model weights saved in ./trained_bert_model/model.safetensors\n",
            "tokenizer config file saved in ./trained_bert_model/tokenizer_config.json\n",
            "Special tokens file saved in ./trained_bert_model/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./trained_bert_model/tokenizer_config.json',\n",
              " './trained_bert_model/special_tokens_map.json',\n",
              " './trained_bert_model/vocab.json',\n",
              " './trained_bert_model/merges.txt',\n",
              " './trained_bert_model/added_tokens.json',\n",
              " './trained_bert_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XM5jQr8ix4mQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}